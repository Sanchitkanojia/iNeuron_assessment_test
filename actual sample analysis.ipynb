{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9bfaea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fc2083b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "      <th>focus_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the genetic changes related to leukoe...</td>\n",
       "      <td>LBSL is caused by mutations in the DARS2 gene,...</td>\n",
       "      <td>GHR</td>\n",
       "      <td>leukoencephalopathy with brainstem and spinal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What to do for Primary Biliary Cirrhosis ?</td>\n",
       "      <td>A healthy diet is important in all stages of c...</td>\n",
       "      <td>NIDDK</td>\n",
       "      <td>Primary Biliary Cirrhosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who is at risk for Fecal Incontinence? ?</td>\n",
       "      <td>Nearly 18 million U.S. adultsabout one in 12ha...</td>\n",
       "      <td>NIDDK</td>\n",
       "      <td>Fecal Incontinence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is (are) Pervasive Developmental Disorders ?</td>\n",
       "      <td>The diagnostic category of pervasive developme...</td>\n",
       "      <td>NINDS</td>\n",
       "      <td>Pervasive Developmental Disorders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the symptoms of Crome syndrome ?</td>\n",
       "      <td>What are the signs and symptoms of Crome syndr...</td>\n",
       "      <td>GARD</td>\n",
       "      <td>Crome syndrome</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What are the genetic changes related to leukoe...   \n",
       "1         What to do for Primary Biliary Cirrhosis ?   \n",
       "2           Who is at risk for Fecal Incontinence? ?   \n",
       "3  What is (are) Pervasive Developmental Disorders ?   \n",
       "4          What are the symptoms of Crome syndrome ?   \n",
       "\n",
       "                                              answer source  \\\n",
       "0  LBSL is caused by mutations in the DARS2 gene,...    GHR   \n",
       "1  A healthy diet is important in all stages of c...  NIDDK   \n",
       "2  Nearly 18 million U.S. adultsabout one in 12ha...  NIDDK   \n",
       "3  The diagnostic category of pervasive developme...  NINDS   \n",
       "4  What are the signs and symptoms of Crome syndr...   GARD   \n",
       "\n",
       "                                          focus_area  \n",
       "0  leukoencephalopathy with brainstem and spinal ...  \n",
       "1                          Primary Biliary Cirrhosis  \n",
       "2                                 Fecal Incontinence  \n",
       "3                  Pervasive Developmental Disorders  \n",
       "4                                     Crome syndrome  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset_path = 'sampled_dataset.csv'  # Change this to your actual file path\n",
    "medquad_df = pd.read_csv(dataset_path)\n",
    "medquad_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c84fbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3282 entries, 0 to 3281\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   question    3282 non-null   object\n",
      " 1   answer      3282 non-null   object\n",
      " 2   source      3282 non-null   object\n",
      " 3   focus_area  3277 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 102.7+ KB\n"
     ]
    }
   ],
   "source": [
    "medquad_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6328c451",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b53a8973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates\n",
    "# Remove duplicate question-answer pairs\n",
    "medquad_df.drop_duplicates(subset=['question', 'answer'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a4757d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3282, 4)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medquad_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "caff6bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Missing Values\n",
    "# Drop rows with any missing values in 'question', 'answer', or 'focus_area' columns\n",
    "medquad_df.dropna(subset=['question', 'answer', 'focus_area'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3277, 4)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medquad_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f2a00f",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c8fc8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Normalization\n",
    "# Convert all text to lower case\n",
    "medquad_df['question'] = medquad_df['question'].str.lower()\n",
    "medquad_df['answer'] = medquad_df['answer'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39264258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advance Text Cleaning\n",
    "import re\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove special characters and digits\n",
    "    text_cleaned = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text_cleaned\n",
    "\n",
    "# Applying text cleaning\n",
    "medquad_df['question'] = medquad_df['question'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "41aefa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Download the necessary NLTK data\n",
    "\n",
    "# Tokenize questions and answers\n",
    "medquad_df['question_tokens'] = medquad_df['question'].apply(word_tokenize)\n",
    "medquad_df['answer_tokens'] = medquad_df['answer'].apply(word_tokenize)\n",
    "## time takes 1min 7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b93d04cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Removal\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from tokens\n",
    "medquad_df['question_tokens'] = medquad_df['question_tokens'].apply(lambda tokens: [w for w in tokens if not w in stop_words])\n",
    "medquad_df['answer_tokens'] = medquad_df['answer_tokens'].apply(lambda tokens: [w for w in tokens if not w in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09bc3035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stemming/Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize tokens\n",
    "medquad_df['question_tokens'] = medquad_df['question_tokens'].apply(lambda tokens: [lemmatizer.lemmatize(w) for w in tokens])\n",
    "medquad_df['answer_tokens'] = medquad_df['answer_tokens'].apply(lambda tokens: [lemmatizer.lemmatize(w) for w in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e285d196",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80a86cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming we're only vectorizing questions for now\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust `max_features` as needed\n",
    "question_vectors = tfidf_vectorizer.fit_transform(medquad_df['question'].values)\n",
    "\n",
    "# 'question_vectors' can now be used as input for machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64391559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "medquad_df['focus_area_encoded'] = label_encoder.fit_transform(medquad_df['focus_area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ea5ba16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(question_vectors, medquad_df['focus_area_encoded'], test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train and y_train can now be used for training a model, and X_test and y_test for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0dd705a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1448170731707317\n",
      "CPU times: total: 9.56 s\n",
      "Wall time: 9.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "## time taken 9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dac700cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores: [0.31238095 0.34923664 0.32633588 0.28625954 0.29007634]\n",
      "CV Accuracy: 0.31 (+/- 0.05)\n",
      "Test Set Evaluation:\n",
      "Accuracy: 0.3232\n",
      "Precision: 0.3277, Recall: 0.3232, F1-Score: 0.3228\n",
      "CPU times: total: 52 s\n",
      "Wall time: 50.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\n",
    "question_vectors = tfidf_vectorizer.fit_transform(medquad_df['question'].values)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    question_vectors, \n",
    "    medquad_df['focus_area_encoded'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Logistic Regression Model\n",
    "model = LogisticRegression(max_iter=1000, C=1.0, class_weight='balanced')  # Adjust C as needed\n",
    "\n",
    "# Training\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "print(f\"Cross-Validation Accuracy Scores: {cv_scores}\")\n",
    "print(f\"CV Accuracy: {np.mean(cv_scores):.2f} (+/- {np.std(cv_scores) * 2:.2f})\")\n",
    "\n",
    "# Evaluation on Test Set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Test Set Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {fscore:.4f}\")\n",
    "\n",
    "## time takes 52s- accuracy- 32s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d2a5650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 547 ms\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming medquad_df['question'] contains your text data and medquad_df['focus_area_encoded'] your labels\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(medquad_df['question'])\n",
    "sequences = tokenizer.texts_to_sequences(medquad_df['question'])\n",
    "padded = pad_sequences(sequences, maxlen=100)  # You might adjust 'maxlen' based on your data\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(medquad_df['focus_area_encoded'])\n",
    "labels = to_categorical(integer_encoded)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4dc9926c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d_3             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d_3             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 173ms/step - accuracy: 0.0000e+00 - loss: 7.7193 - val_accuracy: 0.0000e+00 - val_loss: 7.7147\n",
      "Epoch 2/5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 161ms/step - accuracy: 0.0013 - loss: 7.7088 - val_accuracy: 0.0030 - val_loss: 7.7514\n",
      "Epoch 3/5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 175ms/step - accuracy: 0.0027 - loss: 7.6392 - val_accuracy: 0.0061 - val_loss: 7.7880\n",
      "Epoch 4/5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 157ms/step - accuracy: 0.0047 - loss: 7.5540 - val_accuracy: 0.0030 - val_loss: 8.3953\n",
      "Epoch 5/5\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 158ms/step - accuracy: 0.0020 - loss: 7.4435 - val_accuracy: 0.0046 - val_loss: 8.4297\n",
      "CPU times: total: 1min 32s\n",
      "Wall time: 43.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1eff3fcd990>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2c99d865",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1076\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1076\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:38\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     29\u001b[0m     TFBaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     TFTokenClassifierOutput,\n\u001b[0;32m     37\u001b[0m )\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     39\u001b[0m     TFCausalLanguageModelingLoss,\n\u001b[0;32m     40\u001b[0m     TFMaskedLanguageModelingLoss,\n\u001b[0;32m     41\u001b[0m     TFModelInputType,\n\u001b[0;32m     42\u001b[0m     TFMultipleChoiceLoss,\n\u001b[0;32m     43\u001b[0m     TFNextSentencePredictionLoss,\n\u001b[0;32m     44\u001b[0m     TFPreTrainedModel,\n\u001b[0;32m     45\u001b[0m     TFQuestionAnsweringLoss,\n\u001b[0;32m     46\u001b[0m     TFSequenceClassificationLoss,\n\u001b[0;32m     47\u001b[0m     TFTokenClassificationLoss,\n\u001b[0;32m     48\u001b[0m     get_initializer,\n\u001b[0;32m     49\u001b[0m     keras_serializable,\n\u001b[0;32m     50\u001b[0m     unpack_inputs,\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shape_list, stable_softmax\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py:39\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Repository, list_repo_files\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhdf5_format\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_attributes_to_hdf5_group\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_file_size_to_int, get_checkpoint_shard_files\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.saving.hdf5_format'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, TFBertModel, BertConfig\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Dense, Dropout\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1067\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1066\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1067\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1066\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1066\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1067\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1078\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1076\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1078\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1079\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1081\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode sequences in the dataset\n",
    "X = tokenizer(list(medquad_df['question'].values), padding=True, truncation=True, max_length=100, return_tensors='tf')\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Build the model\n",
    "input_ids = Input(shape=(100,), dtype='int32', name='input_ids')\n",
    "attention_masks = Input(shape=(100,), dtype='int32', name='attention_masks')\n",
    "\n",
    "bert_outputs = bert(input_ids, attention_mask=attention_masks).last_hidden_state\n",
    "clf_output = bert_outputs[:, 0, :]\n",
    "clf_output = Dense(256, activation='relu')(clf_output)\n",
    "clf_output = Dropout(0.2)(clf_output)\n",
    "clf_output = Dense(len(label_encoder.classes_), activation='softmax')(clf_output)\n",
    "\n",
    "model = Model(inputs=[input_ids, attention_masks], outputs=clf_output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Prepare inputs\n",
    "train_inputs = {'input_ids': X['input_ids'], 'attention_masks': X['attention_mask']}\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_inputs, y_train, batch_size=32, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7bbe3f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.4222560975609756\n",
      "Precision: 0.4055, Recall: 0.4223, F1-Score: 0.4094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Instantiate the model with balanced class weights\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')  # Adjust n_estimators as needed\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(y_test, rf_predictions, average='weighted', zero_division=0)\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {fscore:.4f}\")\n",
    "\n",
    "# time takes 3min - accuracy-42%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "448edc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # Adjusting TfidfVectorizer to include more features and n-grams\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=5000,  # Increase max_features\n",
    "#                                    ngram_range=(1, 3))  # Use unigrams, bigrams, and trigrams\n",
    "\n",
    "# # Vectorize the questions with the updated settings\n",
    "# question_vectors = tfidf_vectorizer.fit_transform(medquad_df['question'].values)\n",
    "\n",
    "# # Splitting the dataset into training and testing sets again with the new vectorization\n",
    "# X_train, X_test, y_train, y_test = train_test_split(question_vectors, \n",
    "#                                                     medquad_df['focus_area_encoded'], \n",
    "#                                                     test_size=0.2, \n",
    "#                                                     random_state=42)\n",
    "\n",
    "# # Re-instantiate and train the logistic regression model with the new features\n",
    "# model = LogisticRegression(max_iter=1000)  # Increase max_iter if needed for convergence\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model's performance with the new features\n",
    "# accuracy = model.score(X_test, y_test)\n",
    "# print(f\"Enhanced Model Accuracy: {accuracy}\")\n",
    "\n",
    "# ## time taken 44s minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d8d7bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Instantiate the model\n",
    "# gbm_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)  # Adjust parameters as needed\n",
    "\n",
    "# # Train the model\n",
    "# gbm_model.fit(X_train, y_train) \n",
    "\n",
    "# # Predict on the test set\n",
    "# gbm_predictions = gbm_model.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# gbm_accuracy = accuracy_score(y_test, gbm_predictions)\n",
    "# print(f\"GBM Accuracy: {gbm_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7271ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Assuming 'X_train', 'X_test', 'y_train', 'y_test' are already prepared from the previous TF-IDF vectorization and train-test split\n",
    "\n",
    "# # le = LabelEncoder()\n",
    "# # y_train = le.fit_transform(y_train)\n",
    "# # y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# # Initialize the XGBClassifier\n",
    "# xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', n_estimators=100, learning_rate=0.1, max_depth=6)\n",
    "\n",
    "# # Train the model\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# # Calculate the accuracy\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"XGBoost Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ecd1c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Instantiate the model\n",
    "# nb_model = MultinomialNB()\n",
    "\n",
    "# # Train the model\n",
    "# nb_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# nb_predictions = nb_model.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "# print(f\"Naive Bayes Accuracy: {nb_accuracy}\")\n",
    "\n",
    "# ## time taken 1 min- accuracy- 12%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fae3abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Instantiate the model\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)  # Adjust n_estimators as needed\n",
    "\n",
    "# # Train the model\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "# print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "\n",
    "# ## time takes 44 minutes- accuracy 49%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5337e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Instantiate the model\n",
    "# svm_model = SVC(kernel='linear', C=1)  # Experiment with different kernels and C value\n",
    "\n",
    "# # Train the model\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "# print(f\"SVM Accuracy: {svm_accuracy}\")\n",
    "\n",
    "# ## time taken 96 minutes-accuracy 47%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63540cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb81aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa65bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b31652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
